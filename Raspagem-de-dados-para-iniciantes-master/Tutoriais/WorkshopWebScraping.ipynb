{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WorkshopWebScraping.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DwarfThief/Raspagem-de-dados-para-iniciantes/blob/master/Tutoriais/WorkshopWebScraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ActquKywyCR",
        "colab_type": "text"
      },
      "source": [
        "<div class=\"info\">\n",
        "  <h1 align=\"center\">Workshop de Web Scraping</h1>\n",
        "  <p align=\"center\">Workshop dado por Brenno Barboza para a turma de Inteligência Artificial 2019.1 da UFRPE. <p align=\"center\"> <a href=\"https://docs.google.com/presentation/d/1rgTHkoIPy2TRUaax_RVsQ3466otlt4F-d1H9gfCPc-8/edit?usp=sharing\"> Apresentação</a> do workshop. </p>\n",
        "  </p>\n",
        "</div>\n",
        "\n",
        "![](https://i.creativecommons.org/l/by/4.0/88x31.png)\n",
        "\n",
        "- - -\n",
        "\n",
        "## O que o nosso cliente quer:\n",
        "\n",
        "* Nosso cliente quer todas as ***quotes*** (citações) do site [quotes](http://quotes.toscrape.com).\n",
        "* Juntamente com as **Categorias** e **Autores** das citações.\n",
        "\n",
        "- - -\n",
        "\n",
        "![](https://media.giphy.com/media/3o6ozAc3eCahwy4Cpq/giphy.gif)\n",
        "\n",
        "## Primeiros passos:\n",
        "\n",
        "* Instalar o Scrapy  e criar nossa primeira Spider."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXvWA0OU1Pgm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install scrapy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xEDi5Tn1TVC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!scrapy genspider quotes toscrape.com"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdGnD7VuQwfZ",
        "colab_type": "text"
      },
      "source": [
        "* O código da Spider vai estar assim:\n",
        "\n",
        "```\n",
        "# -*- coding: utf-8 -*-\n",
        "import scrapy\n",
        "\n",
        "\n",
        "class WorkshopSpider(scrapy.Spider):\n",
        "    name = 'Workshop'\n",
        "    allowed_domains = ['toscrape.com']\n",
        "    start_urls = ['http://toscrape.com/']\n",
        "\n",
        "    def parse(self, response):\n",
        "        pass\n",
        "```\n",
        "\n",
        "\n",
        "* Vamos precisar mudar o *`start_urls`* para o link inicial onde a Spider irá acessar.\n",
        "* Agora vamos verificar se a Spider realmente ta acessando o site que queremos, dentro da função `parse` escrevemos *`self.log(\"Estou vivo, visitei\" + response.url)`*\n",
        "* Logo em seguida rodamos a nossa Spider novamente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCJBzJSsTCtl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!scrapy runspider quotes.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5pwLTySbm4e",
        "colab_type": "text"
      },
      "source": [
        "- - -\n",
        "## Agora que sabemos que a nossa Spider ta acessando o site, precisamos começar a coletar o que o cliente deseja.\n",
        "\n",
        "### Vamos analisar a página.\n",
        "* *`response.css('<LocalDeMineração>::<OQueQueremos>').<saida>()*`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRmA6TGUhDFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!scrapy shell http://quotes.toscrape.com/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOEnBt6rhUhc",
        "colab_type": "text"
      },
      "source": [
        "* Como vamos coletar:\n",
        "  * Todos os textos das Citações:\n",
        "    *`response.css('span.text::text').extract()`*\n",
        "  * Todas as tags por Citação:\n",
        "    *`response.css('small.author::text').extract()`*\n",
        "  * Todos os autores das Citações:\n",
        "    *`response.css('a.tag::text').extract()`*\n",
        "\n",
        "### Vamos adicionar essa extração na nossa Spider\n",
        "* Vamos colocar um *yield* e dentro dele inserir as informações que queremos guardar.\n",
        "* Vamos dar nomes aos valores que vamos guardar.\n",
        "\n",
        "### Por final vamos rodar a nossa Spider, mas com uma coisa especial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVj7N-zZkQWF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!scrapy runspider quotes.py -o text.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Z3lo15UPzmu",
        "colab_type": "text"
      },
      "source": [
        "- - -\n",
        "\n",
        "## Melhorando a nossa Spider\n",
        "\n",
        "### Já que não usar o *`extract_first()`* ao coletar as Tags acabamos pegando as Tags de todas as citações.\n",
        "\n",
        "* Precisamos saber de onde vêem as Tags.\n",
        "* Cada categoria ta dentro de um *`<div class=\"quote\">`*\n",
        "* Vamo ver oq existe dentro dessas quotes.\n",
        "* Vamos usar o Scrapy Shell novamente e olhar o que existe dentro dessas citações.\n",
        "  * Escreva *`response.css('div.quote')`*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hO5LSs2Xc5f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!scrapy shell http://quotes.toscrape.com/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfmW-n9_Xpd4",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "* Criar um loop para varrer as **citações**\n",
        "```\n",
        "  for quote in response.css('div.quote'):\n",
        "    coletado = {\n",
        "        'autor': quote.css('small.author::text').extract_first(),\n",
        "        'texto': quote.css('span.text::text').extract_first(),\n",
        "        'categorias': quote.css('a.tag::text').extract(),\n",
        "    }\n",
        "    yield coletado\n",
        "```\n",
        "\n",
        "### Pronto, agora a gente consegue coletar tudo na página.\n",
        "\n",
        "- - - \n",
        "\n",
        "## Agora vamos navegar entre as páginas e coletar tudo do site.\n",
        "\n",
        "![](https://media.tenor.com/images/f1921d017450cd690cfa73bfe33d7724/tenor.gif)\n",
        "\n",
        "* Hora de inspecionar o botão de *Next* da página.\n",
        "* O botão *Next* esta na Tag *`li`* e sua classe é *`next`*, sendo assim vamos extrair dessa maneira:\n",
        "```response.css('li.next')```\n",
        "* O link para a próxima página está na subcategoria `<a>`, então vamos ver o que sai se extrairmos isso e a sua String junto. Escreva isso no Scrapy Shell: *`response.css('li.next > a').extract_first()`*\n",
        ">> O > após o *`li.next`* significa quer queremos o que está dentro dessa subcategoria."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYaomWXUfZdT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!scrapy shell http://quotes.toscrape.com/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjE9HmhqgCoZ",
        "colab_type": "text"
      },
      "source": [
        "* Dentro da subcategoria `<a>` temos acesso ao *`href`*, que é o responsável pelo funcionamento do botão, é isso que queremos.\n",
        "\n",
        ">> ```response.css('li.next > a::attr(href)').extract_first()```\n",
        "\n",
        "> Vamos colocar isso dentro de uma variável e usar essa variável para receber os *`response`* da próxima página, usando o método *`.urljoin()`* com a variável como parâmetro.\n",
        "\n",
        ">> ```response.urljoin(proxima_pag)```\n",
        "\n",
        "> Essa função vai retornar um novo url, vamos guardar isso em outra variável.\n",
        ">> ```proxima_pag = response.css('li.next > a::attr(href)').extract_first()```\n",
        "\n",
        ">> ```proxima_pag_response = response.urljoin(proxima_pag)```\n",
        "\n",
        "> Agora vamos mandar o request para o servidor, receber o response e repetir tudo. Adicionando o método *`Request()`*, passando como parâmetro a variável *`proxima_pag_response`* como a url de acesso e a função para a recursividade, no nosso caso, o parse.\n",
        ">> ```yield scrapy.Request( url = proxima_pag, callback = self.parse)```\n",
        "\n",
        "- - -\n",
        "\n",
        "## Nosso código final deve ficar assim:\n",
        "\n",
        "```\n",
        "# -*- coding: utf-8 -*-\n",
        "import scrapy\n",
        "\n",
        "class QuotesSpider(scrapy.Spider):\n",
        "    name = 'quotes'\n",
        "    allowed_domains = ['toscrape.com']\n",
        "    start_urls = ['http://quotes.toscrape.com']\n",
        "\n",
        "    def parse(self, response):\n",
        "        #Extraindo as citações\n",
        "        for quote in response.css('div.quote'):     \n",
        "            coletado = {\n",
        "                'autor': quote.css('small.author::text').extract_first(),\n",
        "                'texto': quote.css('span.text::text').extract_first(),\n",
        "                'categorias': quote.css('a.tag::text').extract(),\n",
        "            }\n",
        "            yield coletado\n",
        "        #Navegação entre páginas\n",
        "        proxima_pag = response.css('li.next > a::attr(href)').extract_first()\n",
        "        proxima_pag_response = response.urljoin(proxima_pag)\n",
        "        yield scrapy.Request( url = proxima_pag_response, callback = self.parse)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Co1R7ryEi1Q6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!scrapy runspider quotes -o text.json"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}